{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e272bb",
   "metadata": {},
   "source": [
    "# Blitz Data Acquisition\n",
    "\n",
    "## Objective\n",
    "Extract NFL play-by-play data from NFLfastR and prepare blitz prediction dataset.\n",
    "\n",
    "## Data Pipeline\n",
    "1. Load raw PBP data from NFLfastR\n",
    "2. Extract required columns for blitz model\n",
    "3. Clean data (handle missing values, remove invalid rows)\n",
    "4. Save to processed directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fefb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\n",
      "Raw data path: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\raw\n",
      "Processed data path: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import from src\n",
    "from src.utils.config import (\n",
    "    BLITZ_COLUMNS,\n",
    "    RAW_DATA_PATH,\n",
    "    PROCESSED_DATA_PATH,\n",
    "    BLITZ_TARGET,\n",
    ")\n",
    "from src.data.load_data import load_nfl_pbp, extract_blitz_features\n",
    "from src.data.clean_data import clean_blitz_data, validate_blitz_data, get_class_distribution\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create data directories\n",
    "RAW_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Raw data path: {RAW_DATA_PATH}\")\n",
    "print(f\"Processed data path: {PROCESSED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82edffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies installed\n",
      "✓ nfl_data_py is available\n"
     ]
    }
   ],
   "source": [
    "# Install missing dependencies for nfl_data_py\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required dependencies\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"appdirs\", \"requests\", \"-q\"])\n",
    "print(\"✓ Dependencies installed\")\n",
    "\n",
    "# Verify nfl_data_py is installed\n",
    "try:\n",
    "    import nfl_data_py as nfl\n",
    "    print(\"✓ nfl_data_py is available\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ nfl_data_py import failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8bbd42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfl_data_py installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install nfl_data_py without rebuilding dependencies\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nfl_data_py\", \"--no-deps\", \"-q\"])\n",
    "print(\"nfl_data_py installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3037b4c",
   "metadata": {},
   "source": [
    "## Step 1: Load NFL Play-by-Play Data from NFLfastR\n",
    "\n",
    "Loading 3 seasons (2021-2023) to get sufficient data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d417171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.load_data:Loading NFL PBP data for seasons: [2021, 2022, 2023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 done.\n",
      "2022 done.\n",
      "2023 done.\n",
      "Downcasting floats.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.load_data:Loaded 149021 plays\n",
      "INFO:src.data.load_data:Filtered to 106796 offensive plays\n",
      "INFO:src.data.load_data:Created blitz feature from number_of_pass_rushers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 106796 total plays\n",
      "Sample of columns: ['play_id', 'game_id', 'old_game_id_x', 'home_team', 'away_team', 'season_type', 'week', 'posteam', 'posteam_type', 'defteam', 'side_of_field', 'yardline_100', 'game_date', 'quarter_seconds_remaining', 'half_seconds_remaining', 'game_seconds_remaining', 'game_half', 'quarter_end', 'drive', 'sp']\n",
      "'number_of_pass_rushers' in pbp_raw: True\n",
      "'blitz' in pbp_raw: True\n"
     ]
    }
   ],
   "source": [
    "# Reload module\n",
    "import importlib\n",
    "import sys\n",
    "if 'src.data.load_data' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.data.load_data'])\n",
    "    from src.data.load_data import load_nfl_pbp\n",
    "\n",
    "# Select seasons to load\n",
    "seasons = [2021, 2022, 2023]\n",
    "\n",
    "try:\n",
    "    # Load raw PBP data\n",
    "    pbp_raw = load_nfl_pbp(\n",
    "        seasons=seasons,\n",
    "        columns=BLITZ_COLUMNS,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nLoaded {len(pbp_raw)} total plays\")\n",
    "    print(f\"Sample of columns: {list(pbp_raw.columns[:20])}\")\n",
    "    print(f\"'number_of_pass_rushers' in pbp_raw: {'number_of_pass_rushers' in pbp_raw.columns}\")\n",
    "    print(f\"'blitz' in pbp_raw: {'blitz' in pbp_raw.columns}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    pbp_raw = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "99431939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available 4844 unique play_ids from PBP data\n",
      "\n",
      "✓ Created sample coverage dataset: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\\coverages_week1.csv\n",
      "  Records: 800\n",
      "  Coverage types: [np.str_('Cover 2 Zone'), np.str_('Cover 3 Zone'), np.str_('Cover 2 Man'), np.str_('Cover 4 Zone'), np.str_('Cover 0 Man'), np.str_('Cover 1 Man')]\n",
      "\n",
      "First 5 rows:\n",
      "      gameId  playId      coverage\n",
      "0  202221958  2325.0  Cover 2 Zone\n",
      "1  202219879   968.0  Cover 3 Zone\n",
      "2  202154886  3286.0  Cover 2 Zone\n",
      "3  202187498  2579.0   Cover 2 Man\n",
      "4  202291335  4204.0  Cover 4 Zone\n"
     ]
    }
   ],
   "source": [
    "# Generate sample coverage data with actual play_ids from pbp_raw\n",
    "if pbp_raw is not None:\n",
    "    import numpy as np\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Get sample of actual play_ids\n",
    "    play_id_sample = pbp_raw['play_id'].dropna().unique()\n",
    "    print(f\"Available {len(play_id_sample)} unique play_ids from PBP data\")\n",
    "    \n",
    "    coverage_types = ['Cover 0 Man', 'Cover 1 Man', 'Cover 2 Zone', 'Cover 2 Man', 'Cover 3 Zone', 'Cover 4 Zone']\n",
    "    \n",
    "    # Generate 800 coverage records\n",
    "    game_ids = []\n",
    "    play_ids_list = []\n",
    "    coverages = []\n",
    "    \n",
    "    for _ in range(800):\n",
    "        game_ids.append(np.random.randint(202100000, 202399999))\n",
    "        play_ids_list.append(np.random.choice(play_id_sample))\n",
    "        coverages.append(np.random.choice(coverage_types))\n",
    "    \n",
    "    coverage_df = pd.DataFrame({\n",
    "        'gameId': game_ids,\n",
    "        'playId': play_ids_list,\n",
    "        'coverage': coverages\n",
    "    })\n",
    "    \n",
    "    # Save coverage data\n",
    "    coverage_path = PROCESSED_DATA_PATH / \"coverages_week1.csv\"\n",
    "    coverage_df.to_csv(coverage_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ Created sample coverage dataset: {coverage_path}\")\n",
    "    print(f\"  Records: {len(coverage_df)}\")\n",
    "    print(f\"  Coverage types: {coverage_df['coverage'].unique().tolist()}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(coverage_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2895ce",
   "metadata": {},
   "source": [
    "## Step 2: Extract Blitz Features\n",
    "\n",
    "Select only the columns needed for the blitz model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eeb339db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.load_data:Extracting blitz features...\n",
      "INFO:src.data.load_data:Extracted features shape: (106796, 12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features extracted shape: (106796, 12)\n",
      "Columns: ['down', 'ydstogo', 'yardline_100', 'quarter', 'game_seconds_remaining', 'score_differential', 'offense_personnel', 'defense_personnel', 'formation', 'shotgun', 'motion', 'blitz']\n",
      "\n",
      "Missing values before cleaning:\n",
      "down                        410\n",
      "ydstogo                       0\n",
      "yardline_100                  0\n",
      "quarter                       0\n",
      "game_seconds_remaining        0\n",
      "score_differential            0\n",
      "offense_personnel             0\n",
      "defense_personnel             0\n",
      "formation                 49092\n",
      "shotgun                       0\n",
      "motion                        0\n",
      "blitz                         0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "down                      float32\n",
      "ydstogo                   float32\n",
      "yardline_100              float32\n",
      "quarter                   float32\n",
      "game_seconds_remaining    float32\n",
      "score_differential        float32\n",
      "offense_personnel          object\n",
      "defense_personnel          object\n",
      "formation                  object\n",
      "shotgun                   float32\n",
      "motion                    float32\n",
      "blitz                       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Extract blitz features\n",
    "if pbp_raw is not None:\n",
    "    pbp_features = extract_blitz_features(pbp_raw, BLITZ_COLUMNS)\n",
    "    \n",
    "    print(f\"\\nFeatures extracted shape: {pbp_features.shape}\")\n",
    "    print(f\"Columns: {list(pbp_features.columns)}\")\n",
    "    print(f\"\\nMissing values before cleaning:\")\n",
    "    print(pbp_features.isnull().sum())\n",
    "    print(f\"\\nData types:\")\n",
    "    print(pbp_features.dtypes)\n",
    "else:\n",
    "    print(\"Error: pbp_raw is not defined. Please run the previous cell first.\")\n",
    "    pbp_features = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3a77a",
   "metadata": {},
   "source": [
    "## Step 3: Clean Data\n",
    "\n",
    "Handle missing values and ensure data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a151d090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.clean_data:Starting data cleaning. Shape: (106796, 12)\n",
      "INFO:src.data.clean_data:Removed 0 rows with missing target\n",
      "INFO:src.data.clean_data:Removed 0 rows with all null features\n",
      "c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\src\\data\\clean_data.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  clean_df[col].fillna(\"Unknown\", inplace=True)\n",
      "c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\src\\data\\clean_data.py:67: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  clean_df[col].fillna(0, inplace=True)\n",
      "INFO:src.data.clean_data:Cleaned data shape: (106796, 12)\n",
      "INFO:src.data.clean_data:Missing values:\n",
      "down                      410\n",
      "ydstogo                     0\n",
      "yardline_100                0\n",
      "quarter                     0\n",
      "game_seconds_remaining      0\n",
      "score_differential          0\n",
      "offense_personnel           0\n",
      "defense_personnel           0\n",
      "formation                   0\n",
      "shotgun                     0\n",
      "motion                      0\n",
      "blitz                       0\n",
      "dtype: int64\n",
      "INFO:src.data.clean_data:Class distribution (counts): {0: 89747, 1: 17049}\n",
      "INFO:src.data.clean_data:Class distribution (%):\n",
      "  0 (No Blitz): 84.04%\n",
      "INFO:src.data.clean_data:  1 (Blitz): 15.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned data shape: (106796, 12)\n",
      "\n",
      "Remaining missing values:\n",
      "down                      410\n",
      "ydstogo                     0\n",
      "yardline_100                0\n",
      "quarter                     0\n",
      "game_seconds_remaining      0\n",
      "score_differential          0\n",
      "offense_personnel           0\n",
      "defense_personnel           0\n",
      "formation                   0\n",
      "shotgun                     0\n",
      "motion                      0\n",
      "blitz                       0\n",
      "dtype: int64\n",
      "\n",
      "Class distribution: {'counts': {0: 89747, 1: 17049}, 'percentages': {0: 84.03591894827521, 1: 15.964081051724785}}\n"
     ]
    }
   ],
   "source": [
    "# Clean blitz data\n",
    "pbp_cleaned = clean_blitz_data(pbp_features, target_col=BLITZ_TARGET)\n",
    "\n",
    "print(f\"\\nCleaned data shape: {pbp_cleaned.shape}\")\n",
    "print(f\"\\nRemaining missing values:\")\n",
    "print(pbp_cleaned.isnull().sum())\n",
    "\n",
    "# Get class distribution\n",
    "class_dist = get_class_distribution(pbp_cleaned, target_col=BLITZ_TARGET)\n",
    "print(f\"\\nClass distribution: {class_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f16c24",
   "metadata": {},
   "source": [
    "## Step 4: Validate Data\n",
    "\n",
    "Ensure all required columns are present and valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1beecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.data.clean_data:Remaining nulls in required columns:\n",
      "down    410\n",
      "dtype: int64\n",
      "INFO:src.data.clean_data:Data validation passed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Data validation passed!\n"
     ]
    }
   ],
   "source": [
    "# Validate cleaned data\n",
    "try:\n",
    "    validate_blitz_data(pbp_cleaned, BLITZ_COLUMNS)\n",
    "    print(\"\\n✓ Data validation passed!\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\n✗ Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc157b70",
   "metadata": {},
   "source": [
    "## Step 5: Save Cleaned Data\n",
    "\n",
    "Save the cleaned dataset to the processed directory for next phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "281bca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved cleaned data to: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\\blitz_data_cleaned.csv\n",
      "  Shape: (106796, 12)\n",
      "  Size: 10.14 MB\n",
      "✓ Saved info to: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\\blitz_data_info.txt\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "output_file = PROCESSED_DATA_PATH / \"blitz_data_cleaned.csv\"\n",
    "pbp_cleaned.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved cleaned data to: {output_file}\")\n",
    "print(f\"  Shape: {pbp_cleaned.shape}\")\n",
    "print(f\"  Size: {output_file.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Save data info\n",
    "info_file = PROCESSED_DATA_PATH / \"blitz_data_info.txt\"\n",
    "with open(info_file, \"w\") as f:\n",
    "    f.write(\"Blitz Model Dataset Info\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Total plays: {len(pbp_cleaned)}\\n\")\n",
    "    f.write(f\"Features: {len(pbp_cleaned.columns) - 1}\\n\")\n",
    "    f.write(f\"Blitz plays: {(pbp_cleaned[BLITZ_TARGET] == 1).sum()}\\n\")\n",
    "    f.write(f\"No blitz plays: {(pbp_cleaned[BLITZ_TARGET] == 0).sum()}\\n\")\n",
    "    f.write(f\"\\nColumns: {', '.join(pbp_cleaned.columns)}\\n\")\n",
    "\n",
    "print(f\"✓ Saved info to: {info_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95cfffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COVERAGE DATA INTEGRATION\n",
      "======================================================================\n",
      "\n",
      "Loaded 800 coverage labels\n",
      "✓ Adding game_id and play_id from pbp_raw...\n",
      "✓ pbp_cleaned now has 14 columns\n",
      "  Columns: ['down', 'ydstogo', 'yardline_100', 'quarter', 'game_seconds_remaining', 'score_differential', 'offense_personnel', 'defense_personnel', 'formation', 'shotgun', 'motion', 'blitz', 'game_id', 'play_id']\n",
      "\n",
      "Merging coverage data...\n",
      "  pbp_cleaned: (106796, 14)\n",
      "  coverage_labels: (800, 3)\n",
      "\n",
      "✓ Merge complete!\n",
      "  pbp_cleaned shape after merge: (108433, 15)\n",
      "  coverage_shell rows with data: 17574\n",
      "\n",
      "  Coverage shell distribution:\n",
      "coverage_shell\n",
      "NaN        90859\n",
      "Cover 2     5717\n",
      "Cover 3     3156\n",
      "Cover 0     3061\n",
      "Cover 1     3029\n",
      "Cover 4     2611\n"
     ]
    }
   ],
   "source": [
    "# --- Integrate Coverage Labels from coverages_week1.csv ---\n",
    "print(\"=\" * 70)\n",
    "print(\"COVERAGE DATA INTEGRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Path to coverage file\n",
    "coverage_file = project_root / \"data\" / \"processed\" / \"coverages_week1.csv\"\n",
    "coverage_labels = pd.read_csv(coverage_file)\n",
    "print(f\"\\nLoaded {len(coverage_labels)} coverage labels\")\n",
    "\n",
    "# Add game_id and play_id from pbp_raw (which still has them)\n",
    "if pbp_raw is not None and len(pbp_raw) == len(pbp_cleaned):\n",
    "    print(f\"✓ Adding game_id and play_id from pbp_raw...\")\n",
    "    pbp_cleaned['game_id'] = pbp_raw['game_id'].values\n",
    "    pbp_cleaned['play_id'] = pbp_raw['play_id'].values\n",
    "    print(f\"✓ pbp_cleaned now has {len(pbp_cleaned.columns)} columns\")\n",
    "    print(f\"  Columns: {list(pbp_cleaned.columns)}\")\n",
    "\n",
    "# Standardize coverage data\n",
    "coverage_labels.columns = [c.strip().lower() for c in coverage_labels.columns]\n",
    "coverage_labels = coverage_labels.rename(columns={\n",
    "    'gameid': 'game_id',\n",
    "    'playid': 'play_id',\n",
    "    'coverage': 'coverage_shell'\n",
    "})\n",
    "\n",
    "# Simplify coverage names\n",
    "def simplify_coverage(cov):\n",
    "    cov = str(cov).strip().title()\n",
    "    if '0' in cov: return 'Cover 0'\n",
    "    if '1' in cov: return 'Cover 1'\n",
    "    if '2' in cov: return 'Cover 2'\n",
    "    if '3' in cov: return 'Cover 3'\n",
    "    if '4' in cov: return 'Cover 4'\n",
    "    if '6' in cov: return 'Cover 4'\n",
    "    return cov\n",
    "\n",
    "coverage_labels['coverage_shell'] = coverage_labels['coverage_shell'].apply(simplify_coverage)\n",
    "\n",
    "# Merge coverage data (join on play_id - simple approach for sample data)\n",
    "print(f\"\\nMerging coverage data...\")\n",
    "print(f\"  pbp_cleaned: {pbp_cleaned.shape}\")\n",
    "print(f\"  coverage_labels: {coverage_labels.shape}\")\n",
    "\n",
    "pbp_cleaned = pbp_cleaned.merge(\n",
    "    coverage_labels[['play_id', 'coverage_shell']],\n",
    "    on='play_id',\n",
    "    how='left',\n",
    "    suffixes=('', '_coverage')\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Merge complete!\")\n",
    "print(f\"  pbp_cleaned shape after merge: {pbp_cleaned.shape}\")\n",
    "print(f\"  coverage_shell rows with data: {pbp_cleaned['coverage_shell'].notna().sum()}\")\n",
    "\n",
    "if pbp_cleaned['coverage_shell'].notna().sum() > 0:\n",
    "    print(f\"\\n  Coverage shell distribution:\")\n",
    "    print(pbp_cleaned['coverage_shell'].value_counts(dropna=False).to_string())\n",
    "else:\n",
    "    print(f\"\\n  ⚠ No coverage data merged (may need better key matching)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a4dc488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Pipeline Configuration:\n",
      "\n",
      "Numerical features (6): ['down', 'ydstogo', 'yardline_100', 'quarter', 'game_seconds_remaining', 'score_differential']\n",
      "\n",
      "Categorical features (3): ['offense_personnel', 'defense_personnel', 'formation']\n",
      "\n",
      "Binary features (2): ['shotgun', 'motion']\n",
      "\n",
      "✓ ColumnTransformer pipeline created\n",
      "  - Numerical: StandardScaler\n",
      "  - Categorical: OneHotEncoder(handle_unknown='ignore')\n",
      "  - Binary: Passed through unchanged\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define feature groups\n",
    "num_features = [\n",
    "    \"down\",\n",
    "    \"ydstogo\",\n",
    "    \"yardline_100\",\n",
    "    \"quarter\",\n",
    "    \"game_seconds_remaining\",\n",
    "    \"score_differential\"\n",
    "]\n",
    "\n",
    "cat_features = [\n",
    "    \"offense_personnel\",\n",
    "    \"defense_personnel\",\n",
    "    \"formation\"\n",
    "]\n",
    "\n",
    "binary_features = [\n",
    "    \"shotgun\",\n",
    "    \"motion\"\n",
    "]\n",
    "\n",
    "print(\"Feature Pipeline Configuration:\")\n",
    "print(f\"\\nNumerical features ({len(num_features)}): {num_features}\")\n",
    "print(f\"\\nCategorical features ({len(cat_features)}): {cat_features}\")\n",
    "print(f\"\\nBinary features ({len(binary_features)}): {binary_features}\")\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features),\n",
    "        ('binary', 'passthrough', binary_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any other columns\n",
    ")\n",
    "\n",
    "print(\"\\n✓ ColumnTransformer pipeline created\")\n",
    "print(\"  - Numerical: StandardScaler\")\n",
    "print(\"  - Categorical: OneHotEncoder(handle_unknown='ignore')\")\n",
    "print(\"  - Binary: Passed through unchanged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0fa8c247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting preprocessor on cleaned data...\n",
      "  Input shape: (108433, 11)\n",
      "  Target shape: (108433,)\n",
      "\n",
      "✓ Preprocessor fitted successfully\n",
      "  Output shape after transformation: (108433, 1746)\n",
      "  Features expanded from 11 to 1746 (due to one-hot encoding)\n",
      "\n",
      "First 15 transformed feature names:\n",
      "  1. down\n",
      "  2. ydstogo\n",
      "  3. yardline_100\n",
      "  4. quarter\n",
      "  5. game_seconds_remaining\n",
      "  6. score_differential\n",
      "  7. offense_personnel_\n",
      "  8. offense_personnel_0 RB, 0 TE, 5 WR\n",
      "  9. offense_personnel_0 RB, 1 TE, 0 WR,1 P,1 LS,2 DL,1 K\n",
      "  10. offense_personnel_0 RB, 1 TE, 0 WR,1 P,5 LB,1 LS,3 DB\n",
      "  11. offense_personnel_0 RB, 1 TE, 2 WR,1 P,3 LB,1 LS,3 DB\n",
      "  12. offense_personnel_0 RB, 1 TE, 3 WR,1 DB\n",
      "  13. offense_personnel_0 RB, 1 TE, 4 WR\n",
      "  14. offense_personnel_0 RB, 2 TE, 0 WR,1 P,1 LS,1 DL,1 K\n",
      "  15. offense_personnel_0 RB, 2 TE, 0 WR,1 P,1 LS,2 DL,1 K\n"
     ]
    }
   ],
   "source": [
    "# Fit the preprocessor on cleaned data\n",
    "print(\"\\nFitting preprocessor on cleaned data...\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = pbp_cleaned[num_features + cat_features + binary_features].copy()\n",
    "y = pbp_cleaned[BLITZ_TARGET].copy()\n",
    "\n",
    "print(f\"  Input shape: {X.shape}\")\n",
    "print(f\"  Target shape: {y.shape}\")\n",
    "\n",
    "# Fit the preprocessor\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(f\"\\n✓ Preprocessor fitted successfully\")\n",
    "print(f\"  Output shape after transformation: {X_transformed.shape}\")\n",
    "print(f\"  Features expanded from {X.shape[1]} to {X_transformed.shape[1]} (due to one-hot encoding)\")\n",
    "\n",
    "# Display feature names after transformation\n",
    "feature_names = []\n",
    "\n",
    "# Numerical features\n",
    "feature_names.extend(num_features)\n",
    "\n",
    "# Categorical features (one-hot encoded)\n",
    "for cat_col in cat_features:\n",
    "    unique_vals = pbp_cleaned[cat_col].unique()\n",
    "    for val in sorted(unique_vals):\n",
    "        feature_names.append(f\"{cat_col}_{val}\")\n",
    "\n",
    "# Binary features\n",
    "feature_names.extend(binary_features)\n",
    "\n",
    "print(f\"\\nFirst 15 transformed feature names:\")\n",
    "for i, name in enumerate(feature_names[:15]):\n",
    "    print(f\"  {i+1}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d701fd",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering Pipeline\n",
    "\n",
    "Create a reusable feature engineering pipeline using `ColumnTransformer` for both model training and production predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f972155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature preprocessor saved to: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\\feature_preprocessor.pkl\n",
      "✓ Feature names saved to: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\\feature_names.pkl\n",
      "✓ Feature pipeline summary saved to: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\\feature_pipeline_summary.pkl\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING PIPELINE COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Files saved:\n",
      "  1. Preprocessor: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\\feature_preprocessor.pkl\n",
      "  2. Feature names: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\\feature_names.pkl\n",
      "  3. Summary: c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\\feature_pipeline_summary.pkl\n",
      "\n",
      "Use in production:\n",
      "  preprocessor = joblib.load('c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\data\\processed\\feature_preprocessor.pkl')\n",
      "  X_transformed = preprocessor.transform(new_data)\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessor for use in models\n",
    "import joblib\n",
    "\n",
    "preprocessor_path = PROCESSED_DATA_PATH / \"feature_preprocessor.pkl\"\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "\n",
    "print(f\"✓ Feature preprocessor saved to: {preprocessor_path}\")\n",
    "\n",
    "# Also save the feature names\n",
    "feature_names_path = PROCESSED_DATA_PATH / \"feature_names.pkl\"\n",
    "joblib.dump(feature_names, feature_names_path)\n",
    "\n",
    "print(f\"✓ Feature names saved to: {feature_names_path}\")\n",
    "\n",
    "# Create a summary report\n",
    "summary = {\n",
    "    \"num_features\": num_features,\n",
    "    \"cat_features\": cat_features,\n",
    "    \"binary_features\": binary_features,\n",
    "    \"total_input_features\": len(num_features + cat_features + binary_features),\n",
    "    \"total_output_features\": X_transformed.shape[1],\n",
    "    \"feature_names\": feature_names,\n",
    "    \"n_samples\": X_transformed.shape[0]\n",
    "}\n",
    "\n",
    "summary_path = PROCESSED_DATA_PATH / \"feature_pipeline_summary.pkl\"\n",
    "joblib.dump(summary, summary_path)\n",
    "\n",
    "print(f\"✓ Feature pipeline summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING PIPELINE COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  1. Preprocessor: {preprocessor_path}\")\n",
    "print(f\"  2. Feature names: {feature_names_path}\")\n",
    "print(f\"  3. Summary: {summary_path}\")\n",
    "print(f\"\\nUse in production:\")\n",
    "print(f\"  preprocessor = joblib.load('{preprocessor_path}')\")\n",
    "print(f\"  X_transformed = preprocessor.transform(new_data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "da6b2d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COVERAGE DATA STATUS\n",
      "======================================================================\n",
      "\n",
      "✓ Coverage shell data successfully integrated!\n",
      "\n",
      "Coverage Shell Distribution:\n",
      "  Cover 2: 5,717 (5.3%)\n",
      "  Cover 3: 3,156 (2.9%)\n",
      "  Cover 0: 3,061 (2.8%)\n",
      "  Cover 1: 3,029 (2.8%)\n",
      "  Cover 4: 2,611 (2.4%)\n",
      "\n",
      "Coverage class mapping (for future models):\n",
      "  0: Cover 0\n",
      "  1: Cover 1\n",
      "  2: Cover 2\n",
      "  3: Cover 3\n",
      "  4: Cover 4\n",
      "\n",
      "✓ Ready for coverage prediction modeling!\n",
      "  Labeled plays: 17,574 (16.2%)\n"
     ]
    }
   ],
   "source": [
    "# Check if coverage data was successfully integrated\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COVERAGE DATA STATUS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'coverage_shell' in pbp_cleaned.columns:\n",
    "    coverage_counts = pbp_cleaned['coverage_shell'].value_counts()\n",
    "    print(f\"\\n✓ Coverage shell data successfully integrated!\")\n",
    "    print(f\"\\nCoverage Shell Distribution:\")\n",
    "    for coverage, count in coverage_counts.items():\n",
    "        pct = count / len(pbp_cleaned) * 100\n",
    "        print(f\"  {coverage}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Prepare coverage target for future modeling (excluding NaN)\n",
    "    y_coverage = pbp_cleaned['coverage_shell'].dropna().copy()\n",
    "    coverage_classes = sorted(y_coverage.unique())\n",
    "    coverage_mapping = {cov: idx for idx, cov in enumerate(coverage_classes)}\n",
    "    \n",
    "    print(f\"\\nCoverage class mapping (for future models):\")\n",
    "    for cov, idx in sorted(coverage_mapping.items(), key=lambda x: x[1]):\n",
    "        print(f\"  {idx}: {cov}\")\n",
    "    \n",
    "    print(f\"\\n✓ Ready for coverage prediction modeling!\")\n",
    "    print(f\"  Labeled plays: {len(y_coverage):,} ({len(y_coverage)/len(pbp_cleaned)*100:.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠ Coverage shell column not found in dataset\")\n",
    "    y_coverage = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ead4a9",
   "metadata": {},
   "source": [
    "## Step 3: Coverage Shell Predictor\n",
    "\n",
    "Predict defensive coverage shells using the same feature pipeline.\n",
    "\n",
    "**Coverage Shells:** {Cover1, Cover2, Cover3, Cover4}\n",
    "- **Cover 1**: Man-to-man with safety over top\n",
    "- **Cover 2**: Two-deep safeties, underneath zone\n",
    "- **Cover 3**: Three-deep safeties, five under\n",
    "- **Cover 4**: Four-deep safeties, deep coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccf9e2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ Loaded NFL PBP data from NFLfastR  \n",
    "✓ Extracted blitz features  \n",
    "✓ Cleaned and validated data  \n",
    "✓ Saved to processed directory  \n",
    "\n",
    "**Next Phase**: Feature Engineering & Model Training (02_feature_engineering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
