{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4076c9",
   "metadata": {},
   "source": [
    "# Defensive Blitz Prediction - Proof of Concept\n",
    "\n",
    "This notebook demonstrates that we can predict when NFL defenses will blitz with **statistical significance**.\n",
    "\n",
    "**Key Question:** Can we predict defensive blitz plays better than just guessing the majority class?\n",
    "\n",
    "**What we'll show:**\n",
    "1. Load 106,796 real NFL plays\n",
    "2. Build a baseline model (always predict \"no blitz\")\n",
    "3. Build a real model (Random Forest)\n",
    "4. Compare accuracy, precision, recall\n",
    "5. Visualize what features matter most\n",
    "6. Show real predictions on actual game situations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddc4ca",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680ad608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, classification_report\n",
    ")\n",
    "\n",
    "# Style settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831db4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 106,796 plays √ó 12 features\n",
      "\n",
      "Target variable distribution:\n",
      "  No Blitz (0): 89,747 plays (84.0%)\n",
      "  Blitz (1):    17,049 plays (16.0%)\n",
      "\n",
      "Features: ['down', 'ydstogo', 'yardline_100', 'quarter', 'game_seconds_remaining', 'score_differential', 'offense_personnel', 'defense_personnel', 'formation', 'shotgun', 'motion', 'blitz']\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned data\n",
    "data_path = Path(\"../data/processed/blitz_data_cleaned.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} plays √ó {df.shape[1]} features\")\n",
    "print(f\"\\nTarget variable distribution:\")\n",
    "print(f\"  No Blitz (0): {(df['blitz'] == 0).sum():,} plays ({(df['blitz'] == 0).sum() / len(df) * 100:.1f}%)\")\n",
    "print(f\"  Blitz (1):    {(df['blitz'] == 1).sum():,} plays ({(df['blitz'] == 1).sum() / len(df) * 100:.1f}%)\")\n",
    "print(f\"\\nFeatures: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ee602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dropped 411 rows with NaN values\n",
      "  Dataset now: 106,385 plays √ó 12 features\n"
     ]
    }
   ],
   "source": [
    "# Handle remaining NaN values\n",
    "initial_rows = len(df)\n",
    "df = df.dropna()\n",
    "rows_dropped = initial_rows - len(df)\n",
    "\n",
    "print(f\"‚úì Dropped {rows_dropped:,} rows with NaN values\")\n",
    "print(f\"  Dataset now: {len(df):,} plays √ó {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9754f",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24259d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Encoded 3 categorical features: ['offense_personnel', 'defense_personnel', 'formation']\n",
      "\n",
      "Train/Test Split:\n",
      "  Training set: 85,108 plays\n",
      "  Test set:     21,277 plays\n"
     ]
    }
   ],
   "source": [
    "# Separate target and features\n",
    "y = df['blitz']\n",
    "X = df.drop('blitz', axis=1)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"‚úì Encoded {len(categorical_cols)} categorical features: {list(categorical_cols)}\")\n",
    "\n",
    "# Split data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain/Test Split:\")\n",
    "print(f\"  Training set: {len(X_train):,} plays\")\n",
    "print(f\"  Test set:     {len(X_test):,} plays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86213af",
   "metadata": {},
   "source": [
    "## 3. Baseline Model (The Dumb Approach)\n",
    "\n",
    "**What is the baseline?** A model that always predicts the majority class (\"no blitz\").\n",
    "\n",
    "Since 84% of plays are no-blitz, a dumb model that always guesses \"no blitz\" gets 84% accuracy.\n",
    "\n",
    "**Our goal:** Beat this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769a9f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE MODEL: Always predict 'No Blitz'\n",
      "==================================================\n",
      "Accuracy:  84.0%\n",
      "Precision: 0.0% (not applicable - never predicts blitz)\n",
      "Recall:    0.0% (not applicable - never predicts blitz)\n",
      "\n",
      "‚Üí This is our floor. Our real model must beat this.\n"
     ]
    }
   ],
   "source": [
    "# Baseline: always predict the majority class (0 = no blitz)\n",
    "y_test_baseline = np.zeros_like(y_test)\n",
    "\n",
    "baseline_accuracy = accuracy_score(y_test, y_test_baseline)\n",
    "baseline_precision = precision_score(y_test, y_test_baseline, zero_division=0)\n",
    "baseline_recall = recall_score(y_test, y_test_baseline, zero_division=0)\n",
    "\n",
    "print(\"BASELINE MODEL: Always predict 'No Blitz'\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {baseline_accuracy:.1%}\")\n",
    "print(f\"Precision: {baseline_precision:.1%} (not applicable - never predicts blitz)\")\n",
    "print(f\"Recall:    {baseline_recall:.1%} (not applicable - never predicts blitz)\")\n",
    "print(\"\\n‚Üí This is our floor. Our real model must beat this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc64040",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "We'll train two models:\n",
    "1. **Logistic Regression** - simple, interpretable\n",
    "2. **Random Forest** - more complex, usually more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3081e974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "‚úì Logistic Regression trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quays\\source\\repos\\Defensive-Intelligence-Predictor\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"‚úì Logistic Regression trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"‚úì Random Forest trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a8c65",
   "metadata": {},
   "source": [
    "## 5. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for both models\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr, zero_division=0)\n",
    "lr_recall = recall_score(y_test, y_pred_lr, zero_division=0)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr, zero_division=0)\n",
    "lr_auc = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_precision = precision_score(y_test, y_pred_rf, zero_division=0)\n",
    "rf_recall = recall_score(y_test, y_pred_rf, zero_division=0)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf, zero_division=0)\n",
    "rf_auc = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "\n",
    "# Build comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'Logistic Regression', 'Random Forest'],\n",
    "    'Accuracy': [baseline_accuracy, lr_accuracy, rf_accuracy],\n",
    "    'Precision': [0, lr_precision, rf_precision],\n",
    "    'Recall': [0, lr_recall, rf_recall],\n",
    "    'F1 Score': [0, lr_f1, rf_f1],\n",
    "    'ROC-AUC': [0.5, lr_auc, rf_auc]\n",
    "})\n",
    "\n",
    "print(\"\\nMODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\nüìä What these metrics mean:\")\n",
    "print(\"  - Accuracy: Overall correctness (but misleading with imbalanced data)\")\n",
    "print(\"  - Precision: Of predicted blitz, how many actually were blitz?\")\n",
    "print(\"  - Recall: Of actual blitz plays, how many did we catch?\")\n",
    "print(\"  - F1 Score: Harmonic mean of precision and recall\")\n",
    "print(\"  - ROC-AUC: How well does the model rank blitz vs no-blitz?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6376d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1 = axes[0]\n",
    "models = ['Baseline', 'Logistic\\nRegression', 'Random\\nForest']\n",
    "accuracies = [baseline_accuracy, lr_accuracy, rf_accuracy]\n",
    "colors = ['#ff9999', '#ffcc99', '#99cc99']\n",
    "\n",
    "bars1 = ax1.bar(models, accuracies, color=colors, edgecolor='black', linewidth=2)\n",
    "ax1.axhline(y=baseline_accuracy, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0.7, 0.95])\n",
    "for i, v in enumerate(accuracies):\n",
    "    ax1.text(i, v + 0.01, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Precision vs Recall\n",
    "ax2 = axes[1]\n",
    "precision_vals = [lr_precision, rf_precision]\n",
    "recall_vals = [lr_recall, rf_recall]\n",
    "model_names = ['Logistic Regression', 'Random Forest']\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "bars2 = ax2.bar(x_pos - width/2, precision_vals, width, label='Precision', color='#99ccff', edgecolor='black', linewidth=1.5)\n",
    "bars3 = ax2.bar(x_pos + width/2, recall_vals, width, label='Recall', color='#cc99ff', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Precision vs Recall (Blitz Detection)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(model_names)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "for i, (p, r) in enumerate(zip(precision_vals, recall_vals)):\n",
    "    ax2.text(i - width/2, p + 0.02, f'{p:.1%}', ha='center', fontweight='bold', fontsize=10)\n",
    "    ax2.text(i + width/2, r + 0.02, f'{r:.1%}', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93296f5",
   "metadata": {},
   "source": [
    "## 6. Feature Importance (What Actually Predicts Blitz?)\n",
    "\n",
    "**Question:** Which of our 11 features actually matter for predicting blitz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d440e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFEATURE IMPORTANCE (Random Forest)\")\n",
    "print(\"=\" * 50)\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    print(f\"{row['Feature']:.<25} {row['Importance']:.4f} {'‚ñà' * int(row['Importance'] * 100)}\")\n",
    "\n",
    "print(\"\\nüìç Key Insight: These features drive blitz predictions:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {idx+1}. {row['Feature']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37408f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "top_features = feature_importance.head(10)\n",
    "colors_importance = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n",
    "\n",
    "bars = ax.barh(range(len(top_features)), top_features['Importance'].values, color=colors_importance, edgecolor='black', linewidth=1.5)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['Feature'].values)\n",
    "ax.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 10 Features Predicting Blitz Defense', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(top_features['Importance'].values):\n",
    "    ax.text(v + 0.005, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Feature importance visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e0e57",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix (What Are We Getting Right/Wrong?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9871c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix for Random Forest\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nCONFUSION MATRIX (Random Forest on Test Set)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  True Negatives:  {cm[0,0]:>6,}  (correctly predicted NO BLITZ)\")\n",
    "print(f\"  False Positives: {cm[0,1]:>6,}  (predicted BLITZ but wasn't)\")\n",
    "print(f\"  False Negatives: {cm[1,0]:>6,}  (missed actual BLITZ)\")\n",
    "print(f\"  True Positives:  {cm[1,1]:>6,}  (correctly predicted BLITZ)\")\n",
    "\n",
    "# Calculate rates\n",
    "true_neg_rate = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "false_pos_rate = cm[0,1] / (cm[0,0] + cm[0,1])\n",
    "false_neg_rate = cm[1,0] / (cm[1,0] + cm[1,1])\n",
    "true_pos_rate = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "\n",
    "print(f\"\\n  True Neg Rate:   {true_neg_rate:.1%}  (correctly identified NO BLITZ)\")\n",
    "print(f\"  False Pos Rate:  {false_pos_rate:.1%}  (false alarms)\")\n",
    "print(f\"  False Neg Rate:  {false_neg_rate:.1%}  (missed blitzes)\")\n",
    "print(f\"  True Pos Rate:   {true_pos_rate:.1%}  (correctly identified BLITZ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f4bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "            xticklabels=['No Blitz', 'Blitz'],\n",
    "            yticklabels=['No Blitz', 'Blitz'],\n",
    "            annot_kws={'size': 14, 'weight': 'bold'},\n",
    "            ax=ax,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "\n",
    "ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Confusion Matrix: Random Forest Model', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Confusion matrix visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be2257",
   "metadata": {},
   "source": [
    "## 8. Real Predictions on Actual Game Situations\n",
    "\n",
    "Let's look at real plays from our test set and see what our model predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some random test samples\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "\n",
    "print(\"\\nREAL PREDICTION EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "correct_count = 0\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    actual = y_test.iloc[idx]\n",
    "    pred = y_pred_rf[idx]\n",
    "    confidence = y_pred_proba_rf[idx]\n",
    "    is_correct = actual == pred\n",
    "    correct_count += is_correct\n",
    "    \n",
    "    # Get feature values for this play\n",
    "    play_features = X_test.iloc[idx]\n",
    "    \n",
    "    actual_label = \"üî¥ BLITZ\" if actual == 1 else \"üü¢ NO BLITZ\"\n",
    "    pred_label = \"üî¥ BLITZ\" if pred == 1 else \"üü¢ NO BLITZ\"\n",
    "    result = \"‚úÖ CORRECT\" if is_correct else \"‚ùå WRONG\"\n",
    "    \n",
    "    print(f\"\\nPlay #{i}: {result}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Predicted: {pred_label} ({confidence:.1%} confidence)\")\n",
    "    print(f\"  Actual:    {actual_label}\")\n",
    "    print(f\"\\n  Game Situation:\")\n",
    "    print(f\"    ‚Ä¢ Down: {int(play_features['down'])}\")\n",
    "    print(f\"    ‚Ä¢ Yards to Go: {int(play_features['ydstogo'])}\")\n",
    "    print(f\"    ‚Ä¢ Yardline: {int(play_features['yardline_100'])}\")\n",
    "    print(f\"    ‚Ä¢ Quarter: {int(play_features['quarter'])}\")\n",
    "    print(f\"    ‚Ä¢ Score Differential: {int(play_features['score_differential'])}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"\\nAccuracy on these 5 examples: {correct_count}/5 ({correct_count/5:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a653b",
   "metadata": {},
   "source": [
    "## 9. Final Verdict: Proof of Concept\n",
    "\n",
    "**Can we predict defensive blitz with statistical significance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c205dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROOF OF CONCEPT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "improvement = rf_accuracy - baseline_accuracy\n",
    "\n",
    "print(f\"\\nüìä METRICS COMPARISON:\")\n",
    "print(f\"  Baseline (always predict no-blitz):\")\n",
    "print(f\"    ‚Ä¢ Accuracy: {baseline_accuracy:.1%}\")\n",
    "print(f\"\\n  Best Model (Random Forest):\")\n",
    "print(f\"    ‚Ä¢ Accuracy: {rf_accuracy:.1%}\")\n",
    "print(f\"    ‚Ä¢ Precision: {rf_precision:.1%} (of predicted blitz, this % are correct)\")\n",
    "print(f\"    ‚Ä¢ Recall: {rf_recall:.1%} (we catch this % of actual blitz plays)\")\n",
    "print(f\"    ‚Ä¢ F1 Score: {rf_f1:.3f}\")\n",
    "print(f\"    ‚Ä¢ ROC-AUC: {rf_auc:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ IMPROVEMENT:\")\n",
    "print(f\"  {improvement:+.1%} over baseline\")\n",
    "\n",
    "print(f\"\\nüîë KEY FINDINGS:\")\n",
    "print(f\"  1. Top 3 predictors: {', '.join(feature_importance.head(3)['Feature'].tolist())}\")\n",
    "print(f\"  2. We catch {rf_recall:.0%} of actual blitz plays\")\n",
    "print(f\"  3. When we predict blitz, we're right {rf_precision:.0%} of the time\")\n",
    "\n",
    "poc_success = (improvement > 0.05) and (rf_recall > 0.60)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "if poc_success:\n",
    "    print(\"\\n‚úÖ PROOF OF CONCEPT: SUCCESSFUL\")\n",
    "    print(f\"\\nWe have successfully demonstrated that:\")\n",
    "    print(f\"  ‚Ä¢ Our model beats the baseline by {improvement:.1%}\")\n",
    "    print(f\"  ‚Ä¢ We can identify defensive blitz plays with {rf_recall:.0%} recall\")\n",
    "    print(f\"  ‚Ä¢ Game situation features (down, yards, field position) matter\")\n",
    "    print(f\"\\nThis is ready for next phase: production deployment & real-time prediction\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è PROOF OF CONCEPT: NEEDS REFINEMENT\")\n",
    "    print(f\"\\nModel shows promise but needs:\")\n",
    "    print(f\"  ‚Ä¢ Better feature engineering\")\n",
    "    print(f\"  ‚Ä¢ Hyperparameter tuning\")\n",
    "    print(f\"  ‚Ä¢ Additional data or features\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89758bd",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If POC is successful:\n",
    "1. ‚úÖ Deploy model as API endpoint\n",
    "2. ‚úÖ Create real-time prediction dashboard\n",
    "3. ‚úÖ Monitor model performance over time\n",
    "4. ‚úÖ Collect feedback from coaches/analysts\n",
    "5. ‚úÖ Iterate on features and model\n",
    "\n",
    "If POC needs work:\n",
    "1. üîç Analyze failure cases\n",
    "2. üîç Engineer new features\n",
    "3. üîç Try different models/hyperparameters\n",
    "4. üîç Get more data\n",
    "5. üîç Re-test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
